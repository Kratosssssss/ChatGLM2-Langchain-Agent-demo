{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109ad820-a3cc-4c84-ae62-005d8c5c2247",
   "metadata": {},
   "source": [
    "## 第一步：实例化 ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba898de-a2e2-40bf-82c9-bd61da4192e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n",
      "You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009122848510742188,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7157c0d9ec04be4ac7b1da03c93b195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List, Optional, Mapping, Any\n",
    "from functools import partial\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from llm import ChatGLM\n",
    "\n",
    "### chatglm-6B llm ###\n",
    "class ChatGLM(LLM):\n",
    "\n",
    "    model_path: str\n",
    "    max_length: int = 2048\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.7\n",
    "    history: List = []\n",
    "    streaming: bool = True\n",
    "    model: object = None\n",
    "    tokenizer: object = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chatglm-6B\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_path\": self.model_path,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"history\": [],\n",
    "            \"streaming\": self.streaming\n",
    "        }\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        add_history: bool = False\n",
    "    ) -> str:\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\"Must call `load_model()` to load model and tokenizer!\")\n",
    "\n",
    "        if self.streaming:\n",
    "            text_callback = partial(StreamingStdOutCallbackHandler().on_llm_new_token, verbose=True)\n",
    "            resp = self.generate_resp(prompt, text_callback, add_history=add_history)\n",
    "        else:\n",
    "            resp = self.generate_resp(self, prompt, add_history=add_history)\n",
    "\n",
    "        return resp\n",
    "\n",
    "    def generate_resp(self, prompt, text_callback=None, add_history=True):\n",
    "        resp = \"\"\n",
    "        index = 0\n",
    "        if text_callback:\n",
    "            for i, (resp, _) in enumerate(self.model.stream_chat(\n",
    "                    self.tokenizer,\n",
    "                    prompt,\n",
    "                    self.history,\n",
    "                    max_length=self.max_length,\n",
    "                    top_p=self.top_p,\n",
    "                    temperature=self.temperature\n",
    "            )):\n",
    "                if add_history:\n",
    "                    if i == 0:\n",
    "                        self.history += [[prompt, resp]]\n",
    "                    else:\n",
    "                        self.history[-1] = [prompt, resp]\n",
    "                text_callback(resp[index:])\n",
    "                index = len(resp)\n",
    "        else:\n",
    "            resp, _ = self.model.chat(\n",
    "                self.tokenizer,\n",
    "                prompt,\n",
    "                self.history,\n",
    "                max_length=self.max_length,\n",
    "                top_p=self.top_p,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            if add_history:\n",
    "                self.history += [[prompt, resp]]\n",
    "        return resp\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None or self.tokenizer is not None:\n",
    "            return\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True).quantize(8).half().cuda().eval()\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if k in self._identifying_params:\n",
    "                self.k = v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatGLM(model_path=\"./model\")\n",
    "llm.load_model()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e42cd7-4ada-4e84-97dd-7ff9bc2d985b",
   "metadata": {},
   "source": [
    "# 第二步：创建 Controller Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2e7883-8f60-48b9-8a91-fa710a7c703b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import BaseSingleActionAgent\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "\n",
    "\n",
    "class IntentAgent(BaseSingleActionAgent):\n",
    "    tools: List\n",
    "    llm: BaseLanguageModel\n",
    "    intent_template: str = \"\"\"\n",
    "    现在有一些意图，类别为{intents}，你的任务是理解用户问题的意图，并判断该问题属于哪一类意图。\n",
    "    回复的意图类别必须在提供的类别中，并且必须按格式回复：“意图类别：<>”。\n",
    "    \n",
    "    举例：\n",
    "    问题：什么是游戏角色皮卡丘？\n",
    "    意图类别：游戏角色信息查询\n",
    "    \n",
    "    问题：什么是演员刘德华？\n",
    "    意图类别：演员信息查询\n",
    "\n",
    "    问题：“{query}”\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(intent_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def choose_tools(self, query) -> List[str]:\n",
    "        self.get_llm_chain()\n",
    "        tool_names = [tool.name for tool in self.tools]\n",
    "        resp = self.llm_chain.predict(intents=tool_names, query=query)\n",
    "        select_tools = [(name, resp.index(name)) for name in tool_names if name in resp]\n",
    "        select_tools.sort(key=lambda x:x[1])\n",
    "        return [x[0] for x in select_tools]\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "\n",
    "    def plan(\n",
    "            self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        # only for single tool\n",
    "        tool_name = self.choose_tools(kwargs[\"input\"])[0]\n",
    "        return AgentAction(tool=tool_name, tool_input=kwargs[\"input\"], log=\"\")\n",
    "\n",
    "    async def aplan(\n",
    "            self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        raise NotImplementedError(\"IntentAgent does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc350d95-d630-4007-8b75-49789bff0219",
   "metadata": {},
   "source": [
    "# 第三步：创建工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82d98b6-1700-4174-8d0c-033a888a7f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "class functional_Tool(BaseTool):\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    url: str = \"\"\n",
    "\n",
    "    def _call_func(self, query):\n",
    "        raise NotImplementedError(\"subclass needs to overwrite this method\")\n",
    "\n",
    "    def _run(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        return self._call_func(query)\n",
    "\n",
    "    async def _arun(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"APITool does not support async\")\n",
    "        \n",
    "        \n",
    "class Character_knowledge_Tool(functional_Tool):\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    # tool description\n",
    "    name = \"游戏角色信息查询\"\n",
    "    description = \"存有一些角色和信息的工具，输入应该是对游戏角色的询问\"\n",
    "    \n",
    "    # QA params\n",
    "    context = \"已知游戏角色信息：  Mario: 马里奥是日本电子游戏设计师宫本茂创作的一个角色。他是同名电子游戏系列的主角，也是日本电子游戏公司任天堂的吉祥物。Princess Peach: 碧姬公主，是任天堂著名游戏系列马里奥系列中的重要角色。她是游戏中虚构的蘑菇王国的公主，也是王国的统治者。\"\n",
    "    qa_template = \"\"\"\n",
    "    请根据下面带```分隔符的文本来回答问题。\n",
    "    如果该文本中没有相关内容可以回答问题，请直接回复：“抱歉，该问题需要更多上下文信息。”\n",
    "    ```{text}```\n",
    "    问题：{query}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(qa_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def _call_func(self, query) -> str:\n",
    "        self.get_llm_chain()\n",
    "        context = \"已知游戏角色信息：  Mario: 马里奥是日本电子游戏设计师宫本茂创作的一个角色。他是同名电子游戏系列的主角，也是日本电子游戏公司任天堂的吉祥物。Princess Peach: 碧姬公主，是任天堂著名游戏系列马里奥系列中的重要角色。她是游戏中虚构的蘑菇王国的公主，也是王国的统治者。\"\n",
    "        resp = self.llm_chain.predict(text=context, query=query)\n",
    "        return resp\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "            \n",
    "            \n",
    "class Actor_knowledge_Tool(functional_Tool):\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    # tool description\n",
    "    name = \"演员信息查询\"\n",
    "    description = \"存有一些演员的工具，输入应该是对演员的询问\"\n",
    "    \n",
    "    # QA params\n",
    "    qa_template = \"\"\"\n",
    "    请根据下面带```分隔符的文本来回答问题。\n",
    "    如果该文本中没有相关内容可以回答问题，请直接回复：“抱歉，该问题需要更多上下文信息。”\n",
    "    ```{text}```\n",
    "    问题：{query}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(qa_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def _call_func(self, query) -> str:\n",
    "        self.get_llm_chain()\n",
    "        context = \"已知演员信息：  梁朝伟: 1962年6月27日出生于中国香港，祖籍广东台山，华语影视男演员、歌手，国家一级演员, 汤唯: 1979年10月7日出生于浙江省杭州市，毕业于中央戏剧学院导演系本科班，中国内地女演员。\"\n",
    "        resp = self.llm_chain.predict(text=context, query=query)\n",
    "        return resp\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89dc84-efbf-4811-94c9-f4c8479e9c89",
   "metadata": {},
   "source": [
    "# 第四步：试试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73730048-d9eb-483d-8a8d-737550f6013b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "意图类别：游戏角色信息查询\u001b[32;1m\u001b[1;3m\u001b[0m马里奥是是日本电子游戏设计师宫本茂创作的一个角色，他是同名电子游戏系列的主角，也是日本电子游戏公司任天堂的吉祥物。\u001b[36;1m\u001b[1;3m马里奥是是日本电子游戏设计师宫本茂创作的一个角色，他是同名电子游戏系列的主角，也是日本电子游戏公司任天堂的吉祥物。\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "tools = [Character_knowledge_Tool(llm=llm), Actor_knowledge_Tool(llm=llm)]\n",
    "\n",
    "agent = IntentAgent(tools=tools, llm=llm)\n",
    "# agent.choose_tools(\"游戏角色马里奥是谁？\")\n",
    "agent_exec = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, max_iterations=1)\n",
    "agent_exec.run(\"游戏角色马里奥是谁？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
